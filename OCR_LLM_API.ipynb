{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "-iJTqJRdXZjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "clone the repository\n"
      ],
      "metadata": {
        "id": "WUb9fqXKj8sP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88j4vIkhWj0v",
        "outputId": "4288ba23-bbca-4b6f-f9ee-b981c8040485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pdf-contact-api'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 61 (delta 22), reused 51 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (61/61), 26.30 KiB | 3.29 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/alessiobenincasa/pdf-contact-api.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "change directory, libraries and install necessary dependencies"
      ],
      "metadata": {
        "id": "kRpsEOiFXag_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "#change directory\n",
        "cd pdf-contact-api\n",
        "\n",
        "# (Optional) Create and activate a virtual environment - note this may not persist in Colab.\n",
        "# Remove problematic reportlab version from requirements.txt (if necessary)\n",
        "sed -i '/reportlab==3\\.6\\.4/d' requirements.txt\n",
        "\n",
        "# Install dependencies.\n",
        "pip install -r requirements.txt\n",
        "\n",
        "# pip install ngrox\n",
        "pip install pyngrok\n",
        "\n",
        "# Install a compatible version of reportlab (if needed)\n",
        "pip install reportlab==3.6.12\n",
        "\n",
        "# Install llama-cpp-python.\n",
        "pip install llama-cpp-python\n",
        "\n",
        "echo \"Colab setup complete. You can now run the API or test the LLM functionality.\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iXZgfjb-Xaqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb140e91-95e4-41bd-8b20-0a81009a67d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi==0.95.1 (from -r requirements.txt (line 1))\n",
            "  Downloading fastapi-0.95.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvicorn==0.28.0 (from -r requirements.txt (line 2))\n",
            "  Downloading uvicorn-0.28.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting python-multipart==0.0.5 (from -r requirements.txt (line 3))\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 (from fastapi==0.95.1->-r requirements.txt (line 1))\n",
            "  Downloading pydantic-1.10.21-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (153 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 153.9/153.9 kB 9.6 MB/s eta 0:00:00\n",
            "Collecting starlette<0.27.0,>=0.26.1 (from fastapi==0.95.1->-r requirements.txt (line 1))\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn==0.28.0->-r requirements.txt (line 2)) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn==0.28.0->-r requirements.txt (line 2)) (0.14.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from python-multipart==0.0.5->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi==0.95.1->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from starlette<0.27.0,>=0.26.1->fastapi==0.95.1->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi==0.95.1->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi==0.95.1->-r requirements.txt (line 1)) (1.3.1)\n",
            "Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 6.0 MB/s eta 0:00:00\n",
            "Downloading uvicorn-0.28.0-py3-none-any.whl (60 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.6/60.6 kB 6.3 MB/s eta 0:00:00\n",
            "Downloading pydantic-1.10.21-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 64.6 MB/s eta 0:00:00\n",
            "Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 kB 7.2 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: python-multipart\n",
            "  Building wheel for python-multipart (setup.py): started\n",
            "  Building wheel for python-multipart (setup.py): finished with status 'done'\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31668 sha256=f3099964b581e18ae5cd6278250076647e4cab998f7b58f7c16286b93e000c48\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/d4/d0/dbbf3c534e9c5dfbdb2729d10bd12548b8a07103030c07d9e7\n",
            "Successfully built python-multipart\n",
            "Installing collected packages: uvicorn, python-multipart, pydantic, starlette, fastapi\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.6\n",
            "    Uninstalling pydantic-2.10.6:\n",
            "      Successfully uninstalled pydantic-2.10.6\n",
            "Successfully installed fastapi-0.95.1 pydantic-1.10.21 python-multipart-0.0.5 starlette-0.26.1 uvicorn-0.28.0\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: reportlab==3.6.12 in /usr/local/lib/python3.11/dist-packages (3.6.12)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab==3.6.12) (11.1.0)\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Colab setup complete. You can now run the API or test the LLM functionality.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-core 0.3.35 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.21 which is incompatible.\n",
            "albumentations 2.0.4 requires pydantic>=2.9.2, but you have pydantic 1.10.21 which is incompatible.\n",
            "google-genai 0.8.0 requires pydantic<3.0.0dev,>=2.0.0, but you have pydantic 1.10.21 which is incompatible.\n",
            "sigstore-rekor-types 0.0.18 requires pydantic[email]<3,>=2, but you have pydantic 1.10.21 which is incompatible.\n",
            "wandb 0.19.6 requires pydantic<3,>=2.6, but you have pydantic 1.10.21 which is incompatible.\n",
            "langchain 0.3.18 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.21 which is incompatible.\n",
            "sigstore 3.6.1 requires pydantic<3,>=2, but you have pydantic 1.10.21 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNLx09hxz09h",
        "outputId": "8533083b-3300-48d9-a00f-5f108f234a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refactoring of llm.py and adding cli.py"
      ],
      "metadata": {
        "id": "5AS257mWXax3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a file in pdf-contact-api/app named cli.py containing the following code :"
      ],
      "metadata": {
        "id": "tzXR5lZQkNZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Debug output: show current working directory and sys.path before modifying it.\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"Initial sys.path:\", sys.path)\n",
        "\n",
        "# Insert the current working directory (repository root) at the beginning of sys.path.\n",
        "sys.path.insert(0, os.getcwd())\n",
        "\n",
        "# Debug output: show sys.path after modification.\n",
        "print(\"Updated sys.path:\", sys.path)\n",
        "\n",
        "import argparse\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def launch_server():\n",
        "    \"\"\"\n",
        "    Launch the FastAPI server using uvicorn in a subprocess.\n",
        "    The server is defined in app/main.py.\n",
        "    \"\"\"\n",
        "    print(\"Launching API server...\")\n",
        "    proc = subprocess.Popen(\n",
        "        [sys.executable, \"-m\", \"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--reload\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "    )\n",
        "    time.sleep(5)\n",
        "    print(\"Server should be up now at http://localhost:8000\")\n",
        "    print(\"Press Ctrl+C to stop the server.\")\n",
        "    try:\n",
        "        proc.wait()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTerminating server...\")\n",
        "        proc.terminate()\n",
        "        proc.wait()\n",
        "\n",
        "def test_llm(prompt: str):\n",
        "    \"\"\"\n",
        "    Test the LLM extraction function directly using the provided prompt.\n",
        "    \"\"\"\n",
        "    print(\"Testing LLM extraction function...\")\n",
        "    try:\n",
        "        from app.llm import extract_contacts_with_hf\n",
        "        print(\"Successfully imported app.llm!\")\n",
        "    except ImportError as e:\n",
        "        print(\"Error importing LLM module. Make sure your PYTHONPATH is set correctly.\")\n",
        "        print(\"ImportError:\", e)\n",
        "        sys.exit(1)\n",
        "    output = extract_contacts_with_hf(prompt)\n",
        "    print(\"\\n=== LLM Output ===\")\n",
        "    print(output)\n",
        "    print(\"==================\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"CLI for launching and testing the PDF Uploader API.\"\n",
        "    )\n",
        "    group = parser.add_mutually_exclusive_group(required=True)\n",
        "    group.add_argument(\n",
        "        \"--launch\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Launch the API server.\"\n",
        "    )\n",
        "    group.add_argument(\n",
        "        \"--test\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Run a test of the LLM extraction function.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--prompt\",\n",
        "        type=str,\n",
        "        default=\"Contact John Doe at john@example.com or call 123-456-7890.\",\n",
        "        help=\"Prompt text for testing the LLM extraction.\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.launch:\n",
        "        launch_server()\n",
        "    elif args.test:\n",
        "        test_llm(args.prompt)\n",
        "\n"
      ],
      "metadata": {
        "id": "wEb6LBghkVgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "modify llm.py and add instead :"
      ],
      "metadata": {
        "id": "R3H3f_Reka8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# app/llm.py\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Instantiate the model using the from_pretrained method.\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF\",\n",
        "    filename=\"capybarahermes-2.5-mistral-7b.Q2_K.gguf\",\n",
        ")\n",
        "\n",
        "def extract_contacts_with_hf(text: str, max_tokens: int = 128, temperature: float = 0.7, top_p: float = 0.95) -> str:\n",
        "    \"\"\"\n",
        "    Uses the CapybaraHermes chat model to extract contact information from the given text.\n",
        "\n",
        "    The prompt instructs the model to extract emails, phone numbers, addresses, etc.\n",
        "    It returns a JSON formatted string with the extracted contacts.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Extract all the contact information (emails, phone numbers, addresses, etc.) \"\n",
        "        \"from the following text and return it in JSON format:\\n\\n\"\n",
        "        f\"{text}\\n\\n\"\n",
        "        \"JSON:\"\n",
        "    )\n",
        "    try:\n",
        "        # Use create_chat_completion with a single user message.\n",
        "        response = llm.create_chat_completion(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "        )\n",
        "        # The expected response structure is a dict with a 'choices' key.\n",
        "        response_text = response.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
        "        return response_text\n",
        "    except Exception as e:\n",
        "        print(\"Error during LLM query:\", e)\n",
        "        return \"{}\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Multiple sample tests with varying complexity:\n",
        "    sample_tests = [\n",
        "        (\n",
        "            \"Simple Test\",\n",
        "            \"Contact John Doe at john@example.com or call 123-456-7890. \"\n",
        "            \"Office: 987-654-3210, Email: office@example.com.\"\n",
        "        ),\n",
        "        (\n",
        "            \"Moderate Test\",\n",
        "            \"For inquiries, please reach out to Alice Smith at alice.smith@company.com, \"\n",
        "            \"Bob Jones at bob.jones@firm.org, or visit our website. Alternatively, call our \"\n",
        "            \"support hotline at (555) 123-4567.\"\n",
        "        ),\n",
        "        (\n",
        "            \"Complex Test\",\n",
        "            \"Hello, please contact our sales team: John Doe, email: sales@example.com, phone: +44 20 7946 0958; \"\n",
        "            \"for support, contact Jane Roe at jane.roe@support.co.uk or call (800) 555-1212 ext. 99. Also, for HR queries, \"\n",
        "            \"reach out at hr@ourcompany.com. Our offices are located at 123 Business Rd., Business City, BC 12345.\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Run each test and print the extracted contacts.\n",
        "    for test_name, text in sample_tests:\n",
        "        print(f\"--- {test_name} ---\")\n",
        "        contacts = extract_contacts_with_hf(text)\n",
        "        print(\"Extracted contacts:\", contacts)\n",
        "        print(\"\")\n"
      ],
      "metadata": {
        "id": "ZGgaCoDbkeeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "launch and test the app with the following cli :"
      ],
      "metadata": {
        "id": "uHcewj0GkhP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python app/cli.py --test --prompt \"For contact extraction, please process the following information: Our company contacts are as follows: Jane Doe (Email: jane.doe@example.com, Phone: (555) 987-6543, Address: 123 Main St, Springfield); John Smith (Email: john.smith@company.com, Phone: (555) 234-5678, Address: 456 Oak Ave, Metropolis); and Alex Johnson (Email: alex.johnson@enterprise.org, Phone: (555) 345-6789, Address: 789 Pine Rd, Gotham). For urgent issues, contact our support hotline at (555) 111-2222 or email hotline@company.com. Return the extracted details in JSON format.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDMUN5tXhIJv",
        "outputId": "e5e4702c-5229-4721-f1be-b0a52bb73110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/pdf-contact-api\n",
            "Initial sys.path: ['/content/pdf-contact-api/app', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages']\n",
            "Updated sys.path: ['/content/pdf-contact-api', '/content/pdf-contact-api/app', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages']\n",
            "Testing LLM extraction function...\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--CapybaraHermes-2.5-Mistral-7B-GGUF/snapshots/8bea614edd9a2d5d9985a6e6c1ecc166261cacb8/./capybarahermes-2.5-mistral-7b.Q2_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = argilla_capybarahermes-2.5-mistral-7b\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:  129 tensors\n",
            "llama_model_loader: - type q3_K:   64 tensors\n",
            "llama_model_loader: - type q4_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q2_K - Medium\n",
            "print_info: file size   = 2.53 GiB (3.00 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control-looking token:  32000 '<|im_end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special tokens cache size = 5\n",
            "load: token to piece cache size = 0.1637 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.24 B\n",
            "print_info: general.name     = argilla_capybarahermes-2.5-mistral-7b\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32002\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 32000 '<|im_end|>'\n",
            "print_info: EOT token        = 32000 '<|im_end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 32000 '<|im_end|>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q2_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  2592.58 MiB\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 512\n",
            "llama_init_from_model: n_ctx_per_seq = 512\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_init_from_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =    81.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1030\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'argilla_capybarahermes-2.5-mistral-7b', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: chatml\n",
            "Successfully imported app.llm!\n",
            "llama_perf_context_print:        load time =   14863.92 ms\n",
            "llama_perf_context_print: prompt eval time =   14863.57 ms /   247 tokens (   60.18 ms per token,    16.62 tokens per second)\n",
            "llama_perf_context_print:        eval time =   14035.59 ms /   127 runs   (  110.52 ms per token,     9.05 tokens per second)\n",
            "llama_perf_context_print:       total time =   28966.71 ms /   374 tokens\n",
            "\n",
            "=== LLM Output ===\n",
            "{\n",
            "    \"jane_doe\": {\n",
            "        \"email\": \"jane.doe@example.com\",\n",
            "        \"phone\": \"(555) 987-6543\",\n",
            "        \"address\": \"123 Main St, Springfield\"\n",
            "    },\n",
            "    \"john_smith\": {\n",
            "        \"email\": \"john.smith@company.com\",\n",
            "        \"phone\": \"(555) 234-5678\",\n",
            "        \"address\": \"456 Oak Ave, Metropolis\"\n",
            "    },\n",
            "==================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}